{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Watch for changes in any of the imported files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.1599999964237213\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += rewards                                  # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: `brain_name` should be defined globally\n",
    "def env_step(env, actions):\n",
    "    # Execute the action for each agent in the environment\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    # Get the next state for each agent\n",
    "    next_states = env_info.vector_observations\n",
    "    # Get the reward for each agent\n",
    "    rewards = env_info.rewards\n",
    "    # Get whether or not the episode has terminated\n",
    "    dones = env_info.local_done\n",
    "    return next_states, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpg(agent, n_episodes=1000, max_t=1000, eps_start=1.0, \n",
    "               eps_end=0.01, eps_decay=0.995, save_file=None):\n",
    "    \"\"\"Deep Deterministic Policy Gradients.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of time steps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    # All the scores from each episode\n",
    "    scores_all = []\n",
    "    # The last 100 scores\n",
    "    scores_window = deque(maxlen=100)\n",
    "    # Initialize epsilon (the exploration factor)\n",
    "    eps = eps_start\n",
    "    \n",
    "    # Loop over each episode\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # Reset the environment\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        # Get the current state for each agent\n",
    "        states = env_info.vector_observations\n",
    "        # Reset the noise\n",
    "        agent.reset()\n",
    "        # Initialize the score for each agent\n",
    "        scores = np.zeros(num_agents)\n",
    "        \n",
    "        # Loop over each time step\n",
    "        for t in range(max_t):            \n",
    "            # Select an action for each agent according to the current policy\n",
    "            actions = agent.act(states)\n",
    "            # Execute the actions in the environment,\n",
    "            #     then observe the next state and reward for each agent\n",
    "            next_states, rewards, dones = env_step(env, actions)\n",
    "            # Learn from experience and update network parameters for each agent\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            # Update the score for each agent\n",
    "            scores += rewards\n",
    "            # Roll over each state to the next time step\n",
    "            states = next_states\n",
    "            # Record the current mean score across all agents, (overwriting the output)\n",
    "            print(f'\\rEpisode {i_episode}\\tTimestep {t}'\n",
    "                  f'\\tCurrent Score: {np.mean(scores):.2f}', end=\"\")\n",
    "            # Exit the loop if the episode has terminated\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        # Take the average score across all agents\n",
    "        score = np.mean(scores)\n",
    "        # Save the most recent score\n",
    "        scores_all.append(score)\n",
    "        scores_window.append(score)\n",
    "        # Record the mean score over the last 100 scores\n",
    "        mean_score = np.mean(scores_window)\n",
    "        # Decrease epsilon\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "                \n",
    "        # Per episode, record the mean score over the last 100 (or less) episodes\n",
    "        print(f'\\rEpisode {i_episode}'\n",
    "              f'\\tAverage score over the last 100 episodes: {mean_score:.2f}')\n",
    "            \n",
    "        # Goal: Reach 30+ over 100 consecutive episodes\n",
    "        if mean_score >= 30.0:\n",
    "            print(f'\\nEnvironment solved in {i_episode:d} episodes!'\n",
    "                  f'\\tAverage score over the last 100 episodes: {mean_score:.2f}')\n",
    "            if save_file:\n",
    "                torch.save(agent.actor_local.state_dict(), f'{save_file}_actor.pth')\n",
    "                torch.save(agent.critic_local.state_dict(), f'{save_file}_critic.pth')\n",
    "            break\n",
    "            \n",
    "    return scores_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage score over the last 100 episodes: 0.25\n",
      "Episode 2\tAverage score over the last 100 episodes: 0.24\n",
      "Episode 3\tAverage score over the last 100 episodes: 0.41\n",
      "Episode 4\tAverage score over the last 100 episodes: 0.56\n",
      "Episode 5\tAverage score over the last 100 episodes: 0.65\n",
      "Episode 6\tAverage score over the last 100 episodes: 0.72\n",
      "Episode 7\tAverage score over the last 100 episodes: 0.75\n",
      "Episode 8\tAverage score over the last 100 episodes: 0.76\n",
      "Episode 9\tAverage score over the last 100 episodes: 0.80\n",
      "Episode 10\tAverage score over the last 100 episodes: 0.84\n",
      "Episode 11\tAverage score over the last 100 episodes: 0.88\n",
      "Episode 12\tAverage score over the last 100 episodes: 0.91\n",
      "Episode 13\tAverage score over the last 100 episodes: 0.91\n",
      "Episode 14\tAverage score over the last 100 episodes: 0.93\n",
      "Episode 15\tAverage score over the last 100 episodes: 0.97\n",
      "Episode 16\tAverage score over the last 100 episodes: 1.01\n",
      "Episode 17\tAverage score over the last 100 episodes: 1.07\n",
      "Episode 18\tAverage score over the last 100 episodes: 1.14\n",
      "Episode 19\tAverage score over the last 100 episodes: 1.22\n",
      "Episode 20\tAverage score over the last 100 episodes: 1.31\n",
      "Episode 21\tAverage score over the last 100 episodes: 1.40\n",
      "Episode 22\tAverage score over the last 100 episodes: 1.48\n",
      "Episode 23\tAverage score over the last 100 episodes: 1.57\n",
      "Episode 24\tAverage score over the last 100 episodes: 1.71\n",
      "Episode 25\tAverage score over the last 100 episodes: 1.86\n",
      "Episode 26\tAverage score over the last 100 episodes: 1.92\n",
      "Episode 27\tAverage score over the last 100 episodes: 1.96\n",
      "Episode 28\tAverage score over the last 100 episodes: 2.02\n",
      "Episode 29\tAverage score over the last 100 episodes: 2.06\n",
      "Episode 30\tAverage score over the last 100 episodes: 2.15\n",
      "Episode 31\tAverage score over the last 100 episodes: 2.29\n",
      "Episode 32\tAverage score over the last 100 episodes: 2.44\n",
      "Episode 33\tAverage score over the last 100 episodes: 2.59\n",
      "Episode 34\tAverage score over the last 100 episodes: 2.75\n",
      "Episode 35\tAverage score over the last 100 episodes: 2.88\n",
      "Episode 36\tAverage score over the last 100 episodes: 2.99\n",
      "Episode 37\tAverage score over the last 100 episodes: 3.12\n",
      "Episode 38\tAverage score over the last 100 episodes: 3.24\n",
      "Episode 39\tAverage score over the last 100 episodes: 3.34\n",
      "Episode 40\tAverage score over the last 100 episodes: 3.45\n",
      "Episode 41\tAverage score over the last 100 episodes: 3.53\n",
      "Episode 42\tAverage score over the last 100 episodes: 3.67\n",
      "Episode 43\tAverage score over the last 100 episodes: 3.80\n",
      "Episode 44\tAverage score over the last 100 episodes: 3.92\n",
      "Episode 45\tAverage score over the last 100 episodes: 4.02\n",
      "Episode 46\tAverage score over the last 100 episodes: 4.12\n",
      "Episode 47\tAverage score over the last 100 episodes: 4.24\n",
      "Episode 48\tAverage score over the last 100 episodes: 4.34\n",
      "Episode 49\tAverage score over the last 100 episodes: 4.47\n",
      "Episode 50\tAverage score over the last 100 episodes: 4.55\n",
      "Episode 51\tAverage score over the last 100 episodes: 4.65\n",
      "Episode 52\tAverage score over the last 100 episodes: 4.75\n",
      "Episode 53\tAverage score over the last 100 episodes: 4.86\n",
      "Episode 54\tAverage score over the last 100 episodes: 4.99\n",
      "Episode 55\tAverage score over the last 100 episodes: 5.07\n",
      "Episode 56\tAverage score over the last 100 episodes: 5.16\n",
      "Episode 57\tAverage score over the last 100 episodes: 5.23\n",
      "Episode 58\tAverage score over the last 100 episodes: 5.33\n",
      "Episode 59\tAverage score over the last 100 episodes: 5.46\n",
      "Episode 60\tAverage score over the last 100 episodes: 5.61\n",
      "Episode 61\tAverage score over the last 100 episodes: 5.71\n",
      "Episode 62\tAverage score over the last 100 episodes: 5.87\n",
      "Episode 63\tAverage score over the last 100 episodes: 5.98\n",
      "Episode 64\tAverage score over the last 100 episodes: 6.06\n",
      "Episode 65\tAverage score over the last 100 episodes: 6.15\n",
      "Episode 66\tAverage score over the last 100 episodes: 6.20\n",
      "Episode 67\tAverage score over the last 100 episodes: 6.27\n",
      "Episode 68\tAverage score over the last 100 episodes: 6.35\n",
      "Episode 69\tAverage score over the last 100 episodes: 6.42\n",
      "Episode 70\tAverage score over the last 100 episodes: 6.49\n",
      "Episode 71\tAverage score over the last 100 episodes: 6.60\n",
      "Episode 72\tAverage score over the last 100 episodes: 6.66\n",
      "Episode 73\tAverage score over the last 100 episodes: 6.73\n",
      "Episode 74\tAverage score over the last 100 episodes: 6.78\n",
      "Episode 75\tAverage score over the last 100 episodes: 6.81\n",
      "Episode 76\tAverage score over the last 100 episodes: 6.86\n",
      "Episode 77\tAverage score over the last 100 episodes: 6.91\n",
      "Episode 78\tAverage score over the last 100 episodes: 6.97\n",
      "Episode 79\tAverage score over the last 100 episodes: 7.01\n",
      "Episode 80\tAverage score over the last 100 episodes: 7.06\n",
      "Episode 81\tAverage score over the last 100 episodes: 7.13\n",
      "Episode 82\tAverage score over the last 100 episodes: 7.17\n",
      "Episode 83\tAverage score over the last 100 episodes: 7.21\n",
      "Episode 84\tAverage score over the last 100 episodes: 7.27\n",
      "Episode 85\tAverage score over the last 100 episodes: 7.35\n",
      "Episode 86\tAverage score over the last 100 episodes: 7.44\n",
      "Episode 87\tAverage score over the last 100 episodes: 7.52\n",
      "Episode 88\tAverage score over the last 100 episodes: 7.62\n",
      "Episode 89\tAverage score over the last 100 episodes: 7.71\n",
      "Episode 90\tAverage score over the last 100 episodes: 7.77\n",
      "Episode 91\tAverage score over the last 100 episodes: 7.83\n",
      "Episode 92\tAverage score over the last 100 episodes: 7.93\n",
      "Episode 93\tAverage score over the last 100 episodes: 8.01\n",
      "Episode 94\tAverage score over the last 100 episodes: 8.10\n",
      "Episode 95\tAverage score over the last 100 episodes: 8.19\n",
      "Episode 96\tAverage score over the last 100 episodes: 8.22\n",
      "Episode 97\tAverage score over the last 100 episodes: 8.25\n",
      "Episode 98\tAverage score over the last 100 episodes: 8.31\n",
      "Episode 99\tAverage score over the last 100 episodes: 8.38\n",
      "Episode 100\tAverage score over the last 100 episodes: 8.46\n",
      "Episode 101\tAverage score over the last 100 episodes: 8.60\n",
      "Episode 102\tAverage score over the last 100 episodes: 8.73\n",
      "Episode 103\tAverage score over the last 100 episodes: 8.84\n",
      "Episode 104\tAverage score over the last 100 episodes: 8.99\n",
      "Episode 105\tAverage score over the last 100 episodes: 9.17\n",
      "Episode 106\tAverage score over the last 100 episodes: 9.32\n",
      "Episode 107\tAverage score over the last 100 episodes: 9.50\n",
      "Episode 108\tAverage score over the last 100 episodes: 9.65\n",
      "Episode 109\tAverage score over the last 100 episodes: 9.80\n",
      "Episode 110\tAverage score over the last 100 episodes: 9.95\n",
      "Episode 111\tAverage score over the last 100 episodes: 10.14\n",
      "Episode 112\tAverage score over the last 100 episodes: 10.31\n",
      "Episode 113\tAverage score over the last 100 episodes: 10.50\n",
      "Episode 114\tAverage score over the last 100 episodes: 10.66\n",
      "Episode 115\tAverage score over the last 100 episodes: 10.84\n",
      "Episode 116\tAverage score over the last 100 episodes: 11.01\n",
      "Episode 117\tAverage score over the last 100 episodes: 11.20\n",
      "Episode 118\tAverage score over the last 100 episodes: 11.34\n",
      "Episode 119\tAverage score over the last 100 episodes: 11.51\n",
      "Episode 120\tAverage score over the last 100 episodes: 11.69\n",
      "Episode 121\tAverage score over the last 100 episodes: 11.82\n",
      "Episode 122\tAverage score over the last 100 episodes: 11.99\n",
      "Episode 123\tAverage score over the last 100 episodes: 12.15\n",
      "Episode 124\tAverage score over the last 100 episodes: 12.29\n",
      "Episode 125\tAverage score over the last 100 episodes: 12.41\n",
      "Episode 126\tAverage score over the last 100 episodes: 12.57\n",
      "Episode 127\tAverage score over the last 100 episodes: 12.76\n",
      "Episode 128\tAverage score over the last 100 episodes: 12.95\n",
      "Episode 129\tAverage score over the last 100 episodes: 13.12\n",
      "Episode 130\tAverage score over the last 100 episodes: 13.25\n",
      "Episode 131\tAverage score over the last 100 episodes: 13.35\n",
      "Episode 132\tAverage score over the last 100 episodes: 13.47\n",
      "Episode 133\tAverage score over the last 100 episodes: 13.60\n",
      "Episode 134\tAverage score over the last 100 episodes: 13.73\n",
      "Episode 135\tAverage score over the last 100 episodes: 13.84\n",
      "Episode 136\tAverage score over the last 100 episodes: 13.94\n",
      "Episode 137\tAverage score over the last 100 episodes: 14.04\n",
      "Episode 138\tAverage score over the last 100 episodes: 14.13\n",
      "Episode 139\tAverage score over the last 100 episodes: 14.23\n",
      "Episode 140\tAverage score over the last 100 episodes: 14.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 141\tAverage score over the last 100 episodes: 14.45\n",
      "Episode 142\tAverage score over the last 100 episodes: 14.54\n",
      "Episode 143\tAverage score over the last 100 episodes: 14.60\n",
      "Episode 144\tAverage score over the last 100 episodes: 14.69\n",
      "Episode 145\tAverage score over the last 100 episodes: 14.83\n",
      "Episode 146\tAverage score over the last 100 episodes: 14.92\n",
      "Episode 147\tAverage score over the last 100 episodes: 15.03\n",
      "Episode 148\tAverage score over the last 100 episodes: 15.13\n",
      "Episode 149\tAverage score over the last 100 episodes: 15.22\n",
      "Episode 150\tAverage score over the last 100 episodes: 15.32\n",
      "Episode 151\tAverage score over the last 100 episodes: 15.40\n",
      "Episode 152\tAverage score over the last 100 episodes: 15.50\n",
      "Episode 153\tAverage score over the last 100 episodes: 15.59\n",
      "Episode 154\tAverage score over the last 100 episodes: 15.68\n",
      "Episode 155\tAverage score over the last 100 episodes: 15.80\n",
      "Episode 156\tAverage score over the last 100 episodes: 15.92\n",
      "Episode 157\tAverage score over the last 100 episodes: 16.05\n",
      "Episode 158\tAverage score over the last 100 episodes: 16.14\n",
      "Episode 159\tAverage score over the last 100 episodes: 16.22\n",
      "Episode 160\tAverage score over the last 100 episodes: 16.28\n",
      "Episode 161\tAverage score over the last 100 episodes: 16.40\n",
      "Episode 162\tAverage score over the last 100 episodes: 16.46\n",
      "Episode 163\tAverage score over the last 100 episodes: 16.55\n",
      "Episode 164\tAverage score over the last 100 episodes: 16.68\n",
      "Episode 165\tAverage score over the last 100 episodes: 16.81\n",
      "Episode 166\tAverage score over the last 100 episodes: 16.95\n",
      "Episode 167\tAverage score over the last 100 episodes: 17.09\n",
      "Episode 168\tAverage score over the last 100 episodes: 17.19\n",
      "Episode 169\tAverage score over the last 100 episodes: 17.33\n",
      "Episode 170\tAverage score over the last 100 episodes: 17.45\n",
      "Episode 171\tAverage score over the last 100 episodes: 17.58\n",
      "Episode 172\tAverage score over the last 100 episodes: 17.75\n",
      "Episode 173\tAverage score over the last 100 episodes: 17.92\n",
      "Episode 174\tAverage score over the last 100 episodes: 18.08\n",
      "Episode 175\tAverage score over the last 100 episodes: 18.29\n",
      "Episode 176\tAverage score over the last 100 episodes: 18.47\n",
      "Episode 177\tAverage score over the last 100 episodes: 18.67\n",
      "Episode 178\tAverage score over the last 100 episodes: 18.83\n",
      "Episode 179\tAverage score over the last 100 episodes: 19.02\n",
      "Episode 180\tAverage score over the last 100 episodes: 19.18\n",
      "Episode 181\tAverage score over the last 100 episodes: 19.34\n",
      "Episode 182\tAverage score over the last 100 episodes: 19.51\n",
      "Episode 183\tAverage score over the last 100 episodes: 19.66\n",
      "Episode 184\tAverage score over the last 100 episodes: 19.82\n",
      "Episode 185\tAverage score over the last 100 episodes: 19.97\n",
      "Episode 186\tAverage score over the last 100 episodes: 20.13\n",
      "Episode 187\tAverage score over the last 100 episodes: 20.25\n",
      "Episode 188\tAverage score over the last 100 episodes: 20.36\n",
      "Episode 189\tAverage score over the last 100 episodes: 20.48\n",
      "Episode 190\tAverage score over the last 100 episodes: 20.63\n",
      "Episode 191\tAverage score over the last 100 episodes: 20.82\n",
      "Episode 192\tAverage score over the last 100 episodes: 20.96\n",
      "Episode 193\tAverage score over the last 100 episodes: 21.10\n",
      "Episode 194\tAverage score over the last 100 episodes: 21.24\n",
      "Episode 195\tAverage score over the last 100 episodes: 21.37\n",
      "Episode 196\tAverage score over the last 100 episodes: 21.55\n",
      "Episode 197\tAverage score over the last 100 episodes: 21.74\n",
      "Episode 198\tAverage score over the last 100 episodes: 21.90\n",
      "Episode 199\tAverage score over the last 100 episodes: 22.03\n",
      "Episode 200\tAverage score over the last 100 episodes: 22.19\n",
      "Episode 201\tAverage score over the last 100 episodes: 22.32\n",
      "Episode 202\tAverage score over the last 100 episodes: 22.48\n",
      "Episode 203\tAverage score over the last 100 episodes: 22.67\n",
      "Episode 204\tAverage score over the last 100 episodes: 22.80\n",
      "Episode 205\tAverage score over the last 100 episodes: 22.93\n",
      "Episode 206\tAverage score over the last 100 episodes: 23.06\n",
      "Episode 207\tAverage score over the last 100 episodes: 23.19\n",
      "Episode 208\tAverage score over the last 100 episodes: 23.35\n",
      "Episode 209\tAverage score over the last 100 episodes: 23.47\n",
      "Episode 210\tAverage score over the last 100 episodes: 23.63\n",
      "Episode 211\tAverage score over the last 100 episodes: 23.71\n",
      "Episode 212\tAverage score over the last 100 episodes: 23.84\n",
      "Episode 213\tAverage score over the last 100 episodes: 23.95\n",
      "Episode 214\tAverage score over the last 100 episodes: 24.08\n",
      "Episode 215\tAverage score over the last 100 episodes: 24.20\n",
      "Episode 216\tAverage score over the last 100 episodes: 24.31\n",
      "Episode 217\tAverage score over the last 100 episodes: 24.40\n",
      "Episode 218\tAverage score over the last 100 episodes: 24.52\n",
      "Episode 219\tAverage score over the last 100 episodes: 24.62\n",
      "Episode 220\tAverage score over the last 100 episodes: 24.74\n",
      "Episode 221\tAverage score over the last 100 episodes: 24.89\n",
      "Episode 222\tAverage score over the last 100 episodes: 24.99\n",
      "Episode 223\tAverage score over the last 100 episodes: 25.10\n",
      "Episode 224\tAverage score over the last 100 episodes: 25.21\n",
      "Episode 225\tAverage score over the last 100 episodes: 25.34\n",
      "Episode 226\tAverage score over the last 100 episodes: 25.45\n",
      "Episode 227\tAverage score over the last 100 episodes: 25.51\n",
      "Episode 228\tAverage score over the last 100 episodes: 25.56\n",
      "Episode 229\tAverage score over the last 100 episodes: 25.64\n",
      "Episode 230\tAverage score over the last 100 episodes: 25.74\n",
      "Episode 231\tAverage score over the last 100 episodes: 25.88\n",
      "Episode 232\tAverage score over the last 100 episodes: 26.00\n",
      "Episode 233\tAverage score over the last 100 episodes: 26.08\n",
      "Episode 234\tAverage score over the last 100 episodes: 26.14\n",
      "Episode 235\tAverage score over the last 100 episodes: 26.23\n",
      "Episode 236\tAverage score over the last 100 episodes: 26.34\n",
      "Episode 237\tAverage score over the last 100 episodes: 26.46\n",
      "Episode 238\tAverage score over the last 100 episodes: 26.55\n",
      "Episode 239\tAverage score over the last 100 episodes: 26.64\n",
      "Episode 240\tAverage score over the last 100 episodes: 26.80\n",
      "Episode 241\tAverage score over the last 100 episodes: 26.90\n",
      "Episode 242\tAverage score over the last 100 episodes: 27.02\n",
      "Episode 243\tAverage score over the last 100 episodes: 27.17\n",
      "Episode 244\tAverage score over the last 100 episodes: 27.29\n",
      "Episode 245\tAverage score over the last 100 episodes: 27.36\n",
      "Episode 246\tAverage score over the last 100 episodes: 27.51\n",
      "Episode 247\tAverage score over the last 100 episodes: 27.60\n",
      "Episode 248\tAverage score over the last 100 episodes: 27.74\n",
      "Episode 249\tAverage score over the last 100 episodes: 27.84\n",
      "Episode 250\tAverage score over the last 100 episodes: 27.94\n",
      "Episode 251\tAverage score over the last 100 episodes: 28.07\n",
      "Episode 252\tAverage score over the last 100 episodes: 28.17\n",
      "Episode 253\tAverage score over the last 100 episodes: 28.28\n",
      "Episode 254\tAverage score over the last 100 episodes: 28.37\n",
      "Episode 255\tAverage score over the last 100 episodes: 28.48\n",
      "Episode 256\tAverage score over the last 100 episodes: 28.56\n",
      "Episode 257\tAverage score over the last 100 episodes: 28.66\n",
      "Episode 258\tAverage score over the last 100 episodes: 28.75\n",
      "Episode 259\tAverage score over the last 100 episodes: 28.84\n",
      "Episode 260\tAverage score over the last 100 episodes: 28.95\n",
      "Episode 261\tAverage score over the last 100 episodes: 29.03\n",
      "Episode 262\tAverage score over the last 100 episodes: 29.12\n",
      "Episode 263\tAverage score over the last 100 episodes: 29.22\n",
      "Episode 264\tAverage score over the last 100 episodes: 29.29\n",
      "Episode 265\tAverage score over the last 100 episodes: 29.35\n",
      "Episode 266\tAverage score over the last 100 episodes: 29.43\n",
      "Episode 267\tAverage score over the last 100 episodes: 29.51\n",
      "Episode 268\tAverage score over the last 100 episodes: 29.58\n",
      "Episode 269\tAverage score over the last 100 episodes: 29.61\n",
      "Episode 270\tAverage score over the last 100 episodes: 29.69\n",
      "Episode 271\tAverage score over the last 100 episodes: 29.72\n",
      "Episode 272\tAverage score over the last 100 episodes: 29.75\n",
      "Episode 273\tAverage score over the last 100 episodes: 29.80\n",
      "Episode 274\tAverage score over the last 100 episodes: 29.83\n",
      "Episode 275\tAverage score over the last 100 episodes: 29.81\n",
      "Episode 276\tAverage score over the last 100 episodes: 29.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 277\tAverage score over the last 100 episodes: 29.85\n",
      "Episode 278\tAverage score over the last 100 episodes: 29.88\n",
      "Episode 279\tAverage score over the last 100 episodes: 29.88\n",
      "Episode 280\tAverage score over the last 100 episodes: 29.93\n",
      "Episode 281\tAverage score over the last 100 episodes: 29.99\n",
      "Episode 282\tAverage score over the last 100 episodes: 30.03\n",
      "\n",
      "Environment solved in 282 episodes!\tAverage score over the last 100 episodes: 30.03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "agent = Agent(num_agents=num_agents, state_size=state_size, action_size=action_size,\n",
    "              random_seed=0)\n",
    "\n",
    "# Train the DDPG agent\n",
    "scores = train_ddpg(agent, save_file='checkpoint')\n",
    "\n",
    "# Plot the scores\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Trials on Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage score over the last 100 episodes: 0.08\n",
      "Episode 2\tAverage score over the last 100 episodes: 0.25\n",
      "Episode 3\tAverage score over the last 100 episodes: 0.42\n",
      "Episode 4\tAverage score over the last 100 episodes: 0.47\n",
      "Episode 5\tAverage score over the last 100 episodes: 0.56\n",
      "Episode 6\tAverage score over the last 100 episodes: 0.57\n",
      "Episode 7\tAverage score over the last 100 episodes: 0.62\n",
      "Episode 8\tAverage score over the last 100 episodes: 0.70\n",
      "Episode 9\tAverage score over the last 100 episodes: 0.70\n",
      "Episode 10\tAverage score over the last 100 episodes: 0.73\n",
      "Episode 11\tAverage score over the last 100 episodes: 0.68\n",
      "Episode 12\tAverage score over the last 100 episodes: 0.73\n",
      "Episode 13\tAverage score over the last 100 episodes: 0.77\n",
      "Episode 14\tAverage score over the last 100 episodes: 0.81\n",
      "Episode 15\tAverage score over the last 100 episodes: 0.84\n",
      "Episode 16\tAverage score over the last 100 episodes: 0.89\n",
      "Episode 17\tAverage score over the last 100 episodes: 0.92\n",
      "Episode 18\tAverage score over the last 100 episodes: 0.88\n",
      "Episode 19\tAverage score over the last 100 episodes: 0.91\n",
      "Episode 20\tAverage score over the last 100 episodes: 0.94\n",
      "Episode 21\tAverage score over the last 100 episodes: 0.96\n",
      "Episode 22\tAverage score over the last 100 episodes: 0.99\n",
      "Episode 23\tAverage score over the last 100 episodes: 0.99\n",
      "Episode 24\tAverage score over the last 100 episodes: 1.01\n",
      "Episode 25\tAverage score over the last 100 episodes: 1.03\n",
      "Episode 26\tAverage score over the last 100 episodes: 1.05\n",
      "Episode 27\tAverage score over the last 100 episodes: 1.04\n",
      "Episode 28\tAverage score over the last 100 episodes: 1.04\n",
      "Episode 29\tAverage score over the last 100 episodes: 1.06\n",
      "Episode 30\tAverage score over the last 100 episodes: 1.05\n",
      "Episode 31\tAverage score over the last 100 episodes: 1.04\n",
      "Episode 32\tAverage score over the last 100 episodes: 1.06\n",
      "Episode 33\tAverage score over the last 100 episodes: 1.07\n",
      "Episode 34\tAverage score over the last 100 episodes: 1.07\n",
      "Episode 35\tAverage score over the last 100 episodes: 1.07\n",
      "Episode 36\tAverage score over the last 100 episodes: 1.07\n",
      "Episode 37\tAverage score over the last 100 episodes: 1.06\n",
      "Episode 38\tAverage score over the last 100 episodes: 1.05\n",
      "Episode 39\tAverage score over the last 100 episodes: 1.03\n",
      "Episode 40\tAverage score over the last 100 episodes: 1.03\n",
      "Episode 41\tAverage score over the last 100 episodes: 1.04\n",
      "Episode 42\tAverage score over the last 100 episodes: 1.04\n",
      "Episode 43\tAverage score over the last 100 episodes: 1.04\n",
      "Episode 44\tAverage score over the last 100 episodes: 1.04\n",
      "Episode 45\tAverage score over the last 100 episodes: 1.03\n",
      "Episode 46\tAverage score over the last 100 episodes: 1.03\n",
      "Episode 47\tAverage score over the last 100 episodes: 1.02\n",
      "Episode 48\tAverage score over the last 100 episodes: 1.02\n",
      "Episode 49\tAverage score over the last 100 episodes: 1.02\n",
      "Episode 50\tAverage score over the last 100 episodes: 1.01\n",
      "Episode 51\tAverage score over the last 100 episodes: 1.00\n",
      "Episode 52\tTimestep 44\tCurrent Score: 0.05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ba7dc7ba018c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m agent = Agent(num_agents=num_agents, state_size=state_size, action_size=action_size,\n\u001b[1;32m      4\u001b[0m               random_seed=0)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'trial_01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplot_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-014013c1580f>\u001b[0m in \u001b[0;36mtrain_ddpg\u001b[0;34m(agent, n_episodes, max_t, eps_start, eps_end, eps_decay, save_file)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Execute the actions in the environment,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#     then observe the next state and reward for each agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Learn from experience and update network parameters for each agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-46a58a05ae81>\u001b[0m in \u001b[0;36menv_step\u001b[0;34m(env, actions)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Execute the action for each agent in the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get the next state for each agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/control/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/control/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/control/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/control/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/control/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Try 300 and 150 units for the first and second hidden layers, respectively\n",
    "#     (for both actor and critic networks)\n",
    "agent = Agent(num_agents=num_agents, state_size=state_size, action_size=action_size,\n",
    "              random_seed=0)\n",
    "scores = train_ddpg(agent, save_file='trial_01')\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "control"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
